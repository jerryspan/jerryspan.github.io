---
layout: post
title: A long announcement with details
date: 2024-12-19 16:11:00-0400
inline: false
related_posts: false
---

ðŸŽ‰ 3 papers accepted @ COLING 2025! More info below

---

Just before the holiday breakðŸŽ„â„ï¸ and I am delighted to share that some of our latest NLP work is making waves ðŸŒŠ since we got 3(!) papers accepted at the 31st International Conference on Computational Linguistics (COLING 2025 https://lnkd.in/dR8bgtXu)! Some info on the exceptional work of these A-ðŸŒŸ-M-ðŸ’«-A-âœ¨-Z-ðŸ”¥-I-ðŸŽ‰-N-ðŸŒˆ-G-ðŸ‘ PhD candidates below ðŸ‘‡ðŸ‘‡ðŸ‘‡

1/ The ultimate PhD paper of Antoine Louis asks "whether to fuse or not to fuse" in an (legal) IR scenario ðŸ”. BM25 is still a performance beast ðŸ’ª in IR, but it's crucial to know when it shines âœ¨ and where it falls short âš ï¸ compared to dense models. In the paper we explore different scenarios and conclude:
â— BM25 = still the ðŸŽ of search, esp. in zero-shot tasks or when efficiency rules.
â— Fusing models? ðŸ¤ Great for zero-shotâ€”boosts general IR models
â— Got domain-specific data? ðŸ§  Fine-tune one model forbest results.

ðŸ“‘ Paper: https://lnkd.in/dxr2VQQE (w/Gijs van Dijck)
ðŸ’» Code: https://lnkd.in/d8wrEP7i
ðŸ¤— Models: https://lnkd.in/d4RwVVfc

2/ Have you noticed how most information retrieval work is in English and Chinese? Well, Antoine and Vageesh noticed the same and as a PhD-side-project worked on delivering ColBERT-XM ðŸŒ a modular retriever for 81+ languages ðŸ§© Built w/XMOD encoders & ColBERTâ€™s backbone, it trains on English (high-resource language) and transfers zero-shot to other languages, thereby eliminating the need for language-specific labeled retrieval data.ðŸ’¡âœ¨

ðŸ“‘ Paper: https://lnkd.in/dXHKunum (w/Gijs van Dijck)
ðŸ’» Code: https://lnkd.in/dw4N5PdP
ðŸ¤— Model: https://lnkd.in/dHpAN5yR

3/ PaweÅ‚ MÄ…ka's 2nd PhD paper dives into: how do context-aware machine translation models really use context? ðŸ¤” We analyzed attention heads and found:
ðŸ”‘ Some are critical for pronoun disambiguation.
ðŸš€ Fine-tuning these heads = boosted performance!
This work builds on VOXReality EU project, where we -efficiently- integrate SoTA MT models in AR/VR ðŸ•¶ï¸ðŸŒðŸŽ® scenarios, therefore context use is essential.

ðŸ“‘ Paper: https://lnkd.in/dc9sVYtn (w/Yusuf Can Semerci, Johannes (Jan) C. Scholtes)
ðŸ’» Code: https://lnkd.in/duNdb5YYt grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
> â€”Anais Nin

Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.
