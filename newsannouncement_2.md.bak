---
layout: post
title: 3 papers accepted @ COLING 2025! Click for more!
date: 2024-12-19 16:11:00-0400
inline: false
related_posts: false
---

ğŸ‰ 3 papers accepted @ COLING 2025!

---

Just before the holiday breakğŸ„â„ï¸ and I am delighted to share that some of our latest NLP work is making waves ğŸŒŠ since we got 3(!) papers accepted at the 31st International Conference on Computational Linguistics (COLING 2025 https://lnkd.in/dR8bgtXu)! Some info on the exceptional work of these A-ğŸŒŸ-M-ğŸ’«-A-âœ¨-Z-ğŸ”¥-I-ğŸ‰-N-ğŸŒˆ-G-ğŸ‘ PhD candidates below ğŸ‘‡ğŸ‘‡ğŸ‘‡

1/ The ultimate PhD paper of Antoine Louis asks "whether to fuse or not to fuse" in an (legal) IR scenario ğŸ”. BM25 is still a performance beast ğŸ’ª in IR, but it's crucial to know when it shines âœ¨ and where it falls short âš ï¸ compared to dense models. In the paper we explore different scenarios and conclude:
â— BM25 = still the ğŸ of search, esp. in zero-shot tasks or when efficiency rules.
â— Fusing models? ğŸ¤ Great for zero-shotâ€”boosts general IR models
â— Got domain-specific data? ğŸ§  Fine-tune one model forbest results.

ğŸ“‘ Paper: https://lnkd.in/dxr2VQQE (w/Gijs van Dijck)
ğŸ’» Code: https://lnkd.in/d8wrEP7i
ğŸ¤— Models: https://lnkd.in/d4RwVVfc

2/ Have you noticed how most information retrieval work is in English and Chinese? Well, Antoine and Vageesh noticed the same and as a PhD-side-project worked on delivering ColBERT-XM ğŸŒ a modular retriever for 81+ languages ğŸ§© Built w/XMOD encoders & ColBERTâ€™s backbone, it trains on English (high-resource language) and transfers zero-shot to other languages, thereby eliminating the need for language-specific labeled retrieval data.ğŸ’¡âœ¨

ğŸ“‘ Paper: https://lnkd.in/dXHKunum (w/Gijs van Dijck)
ğŸ’» Code: https://lnkd.in/dw4N5PdP
ğŸ¤— Model: https://lnkd.in/dHpAN5yR

3/ PaweÅ‚ MÄ…ka's 2nd PhD paper dives into: how do context-aware machine translation models really use context? ğŸ¤” We analyzed attention heads and found:
ğŸ”‘ Some are critical for pronoun disambiguation.
ğŸš€ Fine-tuning these heads = boosted performance!
This work builds on VOXReality EU project, where we -efficiently- integrate SoTA MT models in AR/VR ğŸ•¶ï¸ğŸŒğŸ® scenarios, therefore context use is essential.

ğŸ“‘ Paper: https://lnkd.in/dc9sVYtn (w/Yusuf Can Semerci, Johannes (Jan) C. Scholtes)
ğŸ’» Code: https://lnkd.in/duNdb5YY
